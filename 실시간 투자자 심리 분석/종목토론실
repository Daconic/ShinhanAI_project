import streamlit as st
import pandas as pd
import numpy as np

import requests
from bs4 import BeautifulSoup
import re

import FinanceDataReader as fdr


# í•œí˜ì´ì§€ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
def get_one_page(item_code, page_no):
    url = f"https://finance.naver.com/item/board.naver?code={item_code}&page={page_no}"
    headers = {"user-agent":"Mozilla/5.0"}
    response = requests.get(url, headers=headers)

    table = pd.read_html(response.text)[1]
    cols = ["ë‚ ì§œ", "ì œëª©", "ê¸€ì“´ì´", "ì¡°íšŒ", "ê³µê°", "ë¹„ê³µê°"]
    table = table[cols]
    temp = table.dropna()
    return temp

# ì—¬ëŸ¬ í˜ì´ì§€ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
def jongmok_toron(item_code, last_page):
    
    post_list = []
    for page_no in range(1, last_page + 1):

        df = get_one_page(item_code, page_no)
        post_list.append(df) 

    post_list = pd.concat(post_list, ignore_index=True)
    return post_list

# ë‚´ìš© ë§í¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
def jongmok_context(item_code, page_no):
    url = f'https://finance.naver.com/item/board.naver?code={item_code}&page={page_no}'
    headers = {"user-agent":"Mozilla/5.0"}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    base_url = 'https://finance.naver.com/'
    sub_url_list = [soup.select('td.title > a')[i]['href'] for i in range(len(soup.select('td.title > a')))]
    context_url_list = [base_url + i for i in sub_url_list]
    return context_url_list

# ì›í•˜ëŠ” í˜ì´ì§€ë§Œí¼ ë‚´ìš© ë§í¬ë¥¼ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜ ë§Œë“¤ê¸°
def context_link(item_code, last_page):
    b=[]
    for i in range(1, last_page + 1):
        a = jongmok_context(item_code, i)
        b.append(a)
        context_list = pd.concat([pd.DataFrame(x) for x in b], ignore_index=True)
    return context_list

# ì¢…ëª©í† ë¡  ëª©ë¡ê³¼ ë‚´ìš©ë§í¬ë¥¼ í¬í•¨í•œ ë°ì´í„°í”„ë ˆì„ ë§Œë“¤ê¸°
def ëª©ë¡_ë‚´ìš©ë§í¬(item_code, last_page):
    ì¢…ëª©í† ë¡ _ëª©ë¡ = jongmok_toron(item_code, last_page)
    ì¢…ëª©í† ë¡ _ë‚´ìš©ë§í¬ = context_link(item_code, last_page)
    a = pd.concat([ì¢…ëª©í† ë¡ _ëª©ë¡, ì¢…ëª©í† ë¡ _ë‚´ìš©ë§í¬], axis=1, ignore_index=True)
    a.columns = ['ë‚ ì§œ', 'ì œëª©', 'ê¸€ì“´ì´', 'ì¡°íšŒ', 'ê³µê°', 'ë¹„ê³µê°', 'ë‚´ìš©ë§í¬']
    return a


# ë‚´ìš©ì„ ìˆ˜ì§‘í•˜ëŠ” í•¨ìˆ˜
def ì¢…ëª©í† ë¡ _ë‚´ìš©(item_code, last_page):
    content = []
    df = ëª©ë¡_ë‚´ìš©ë§í¬(item_code, last_page)
    for i in df['ë‚´ìš©ë§í¬']:
        headers = {"user-agent": "Mozilla/5.0"}
        response = requests.get(i, headers=headers)
        html = BeautifulSoup(response.text)
        a = html.select('#body')[0].text
        content.append(a)        
    return content

def ì¢…ëª©í† ë¡ _ëª©ë¡_ë‚´ìš©ë§í¬_ë‚´ìš©(item_code, last_page):
    df = ëª©ë¡_ë‚´ìš©ë§í¬(item_code, last_page)
    df['ë‚´ìš©'] = ì¢…ëª©í† ë¡ _ë‚´ìš©(item_code, last_page)
    return df

# í† í°í™”
from transformers import PreTrainedTokenizerFast

tokenizer = PreTrainedTokenizerFast.from_pretrained(
    "skt/kogpt2-base-v2",
    bos_token='</s>', 
    eos_token='</s>', 
    unk_token='<unk>',
    pad_token='<pad>', 
    mask_token='<mask>')
    
# ê°ì„±ë¶„ì„
from transformers import pipeline

classifier = pipeline("sentiment-analysis")

def ì¢…ëª©í† ë¡ _ì œëª©_ë‚´ìš©ë§í¬_ë‚´ìš©_ê°ì„±ë¶„ì„(item_code, last_page):
    df = ì¢…ëª©í† ë¡ _ëª©ë¡_ë‚´ìš©ë§í¬_ë‚´ìš©(item_code, last_page)
    classifier = pipeline("sentiment-analysis")
    ì œëª©ë‚´ìš© = [df['ì œëª©'][i] + ' ' + df['ë‚´ìš©'][i] for i in range(len(df))]

    # ì •ê·œí‘œí˜„ì‹ ì ìš©
    ì œëª©ë‚´ìš©_íŠ¹ìˆ˜ë¬¸ìì œê±° = []
    for i in ì œëª©ë‚´ìš©:
        w = re.sub(r"[^ê°€-í£\s\d]", "", i)
        ì œëª©ë‚´ìš©_íŠ¹ìˆ˜ë¬¸ìì œê±°.append(w)

    # í† í°í™”
    ì œëª©ë‚´ìš©_íŠ¹ìˆ˜ë¬¸ìì œê±°_í† í°í™” = []
    for i in ì œëª©ë‚´ìš©_íŠ¹ìˆ˜ë¬¸ìì œê±°:
        a = tokenizer.tokenize(i)
        ì œëª©ë‚´ìš©_íŠ¹ìˆ˜ë¬¸ìì œê±°_í† í°í™”.append(a)

    # ê°ì„±ë¶„ì„
    sentiment = []
    for i in ì œëª©ë‚´ìš©_íŠ¹ìˆ˜ë¬¸ìì œê±°_í† í°í™”:
        a = classifier(i)
        sentiment.append(a)

    # ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë§Œë“¤ê¸°
    b = []
    for i in range(len(sentiment)):
        a = pd.DataFrame(sentiment[i], columns=['label', 'score'])
        b.append(a)
        sentiment_analysis = pd.concat(b, ignore_index=True)

    df['sentiment-analysis'] = sentiment_analysis['label']
    df['score'] = sentiment_analysis['score']

    return df

def ì¢…ëª©ë³„_ê¸ì •ì ìˆ˜(item_code, last_page):
    data = ì¢…ëª©í† ë¡ _ì œëª©_ë‚´ìš©ë§í¬_ë‚´ìš©_ê°ì„±ë¶„ì„(item_code, last_page)

    ê¸ì •ì ìˆ˜ = []
    for i in range(len(data[['sentiment-analysis', 'score']])):
        if data['sentiment-analysis'][i] == 'POSITIVE':
            a = data['score'][i]
            ê¸ì •ì ìˆ˜.append(a)
        else:
            a = 1 - data['score'][i]
            ê¸ì •ì ìˆ˜.append(a)
    data['ê¸ì •ì ìˆ˜'] = ê¸ì •ì ìˆ˜
    return data['ê¸ì •ì ìˆ˜'].mean()


item_code = st.text_input('ë¶„ì„ì„ ì›í•˜ì‹œëŠ” ì¢…ëª©ì˜ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”.')
last_page = 1
# last_page = st.number_input('ìŠ¤í¬ë˜í•‘í•  í˜ì´ì§€ ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš”. (í˜ì´ì§€ ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ì •í™•ë„ëŠ” í–¥ìƒë˜ì§€ë§Œ ì˜¤ëœ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤)', min_value=0, step=1, format='%d')

score = round(float(ì¢…ëª©ë³„_ê¸ì •ì ìˆ˜(item_code, last_page)), 3)

# ì¢…ëª©ëª… ì¶œë ¥
krx = fdr.StockListing('KRX')
name = krx[krx['Code'] == item_code]['Name'].iloc[0]
    
print(f' ì…ë ¥í•˜ì‹  ì¢…ëª©ì€ {name}({item_code})ì…ë‹ˆë‹¤.')
print(f" ë„¤ì´ë²„ ì¢…ëª©í† ë¡ ì‹¤ì—ì„œ í•´ë‹¹ ì¢…ëª©ì˜ ì‹¤ì‹œê°„ ê¸ì • ì ìˆ˜ëŠ” {score}ì…ë‹ˆë‹¤.")
print(' ì €í¬ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤.')
print('ğŸ’¡íˆ¬ìì˜ ì±…ì„ì€ ë³¸ì¸ì—ê²Œ ìˆìŠµë‹ˆë‹¤.ğŸ’¡')


